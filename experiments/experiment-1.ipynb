{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc2d6a3d-bafd-4a10-8d0d-31fcd28d73f5",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7126085c-cd44-4dc8-a87f-4fcf858f2cbe",
   "metadata": {},
   "source": [
    "These experiments aim to determine the baseline performance by classifying a test set of standalone question utterances, with (size 90) and without a training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea69a7c-520e-44d8-a3cb-f2ad109b0ac6",
   "metadata": {},
   "source": [
    "# Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e9c4da2-4a1d-4785-afaa-a1432f2865e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import openai\n",
    "import tiktoken\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import calculate_openai_cost\n",
    "\n",
    "# load environment variables from .env\n",
    "load_dotenv()  \n",
    "\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Initialize the OpenAI client w/API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# https://platform.openai.com/docs/models\n",
    "GPT_MODEL = os.getenv(\"OPENAI_MODEL\")\n",
    "\n",
    "# https://openai.com/api/pricing/\n",
    "PROMPT_COST_PER_1000 = os.getenv(\"PROMPT_COST_PER_1000\")\n",
    "COMPLETION_COST_PER_1000 = os.getenv(\"COMPLETION_COST_PER_1000\")\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "DATA_FILE = os.getenv(\"DATA_FILE\")\n",
    "\n",
    "# dataset ref: https://doi.org/10.1017/pds.2023.100\n",
    "data_path_qa = Path(\"../\", DATA_DIR, DATA_FILE)\n",
    "\n",
    "df = pd.read_excel(data_path_qa, usecols=\"F,G,H,O\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32fda76-3048-4960-818a-43d45c268547",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db66baa0-df2d-40f1-b998-f212f35ca5a7",
   "metadata": {},
   "source": [
    "[Relevant Paper](https://doi.org/10.1017/pds.2023.100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78a76734-aa99-4ad4-9240-989157ba3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate DataFrames for each class\n",
    "llq_data = df[df['A-General Type of Questions'] == 'LLQ']\n",
    "gdq_data = df[df['A-General Type of Questions'] == 'GDQ']\n",
    "drq_data = df[df['A-General Type of Questions'] == 'DRQ']\n",
    "\n",
    "# Sample 30% of LLQ, 30% of GDQ, and 30% of DRQ for training\n",
    "llq_train, llq_test = train_test_split(llq_data, test_size=0.3, random_state=SEED)\n",
    "gdq_train, gdq_test = train_test_split(gdq_data, test_size=0.3, random_state=SEED)\n",
    "drq_train, drq_test = train_test_split(drq_data, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Concatenate the sampled data for training\n",
    "train_sample = pd.concat([llq_train.head(30), gdq_train.head(30), drq_train.head(30)])\n",
    "\n",
    "# Concatenate the sampled data for testing\n",
    "test_sample = pd.concat([llq_test.head(10), gdq_test.head(10), drq_test.head(10)])\n",
    "\n",
    "# Shuffle the training and test samples\n",
    "train_sample = train_sample.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "test_sample = test_sample.sample(frac=1, random_state=SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1548bbbc-d671-4c21-beef-237d54e8cb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A-General Type of Questions\n",
       "LLQ    1322\n",
       "GDQ     536\n",
       "DRQ     174\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['A-General Type of Questions'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f155e1-51a3-4462-9e28-aa3aeac83f1f",
   "metadata": {},
   "source": [
    "# With Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb62a9c7-3a31-4a49-96ca-d06457fc03f0",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdca3798-8fa8-4c1b-a9d7-319bf114572f",
   "metadata": {},
   "source": [
    "### System message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e4d01cb-ba63-4e41-acd5-2f338ddd8a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Message Input token size: 1006\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(GPT_MODEL)\n",
    "\n",
    "with open(\"../system-message.txt\") as f:\n",
    "    persona_text = f.read()\n",
    "    print(f\"System Prompt Message Input token size: {len(encoding.encode(persona_text))}\")\n",
    "\n",
    "\n",
    "# NB \n",
    "# 1. The content is collected from [https://doi.org/10.1016/j.destud.2016.07.002] Appendix 1\n",
    "# 2. In the prompt, Right/Left Double Quotation Mark (“ ”) to quote Eris instead of Straight Double Quotation Mark (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd7efb5-24c3-4695-a182-d2bfd0f3811b",
   "metadata": {},
   "source": [
    "### User message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3ad2c4-fcc3-4902-a62d-bd90da09fe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 2118\n"
     ]
    }
   ],
   "source": [
    "# create train example list\n",
    "train_sample['example'] = train_sample['Questions'] \\\n",
    "                        + ' : ' + train_sample[\"A-General Type of Questions\"] +'\\n' \n",
    "example_txt = ''.join(train_sample['example'])\n",
    "\n",
    "# numbered test qs list\n",
    "ques_list = '\\n'.join(test_sample['Questions'])\n",
    "num_ques_list = '\\n'.join(f\"{i+1}. {question}\" for i, question in enumerate(ques_list.split('\\n')))\n",
    "\n",
    "prompt_train = f\"Classify each of the questions below, delimited by triple backticks, using the taxonomy proposed by Eris. \\\n",
    "Label each question with one of the three categories: Low-level questions, Deep Reasoning Questions, or Generative Design Questions. \\\n",
    "State your reasoning for the assigned label. Format the result as a markdown table.\\n\\\n",
    "```\\n{num_ques_list}\\n```\\nTo help you categorize the questions above, here are some examples \\\n",
    "delimited by triple backticks, each line contains an example that has two segments - question \\\n",
    "and category separated by colon (:)\\n\\n```\\n{example_txt}\\n```\"\n",
    "\n",
    "\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb0e20-dc94-464a-9f8f-3b940cd38eac",
   "metadata": {},
   "source": [
    "## Experiment 1A\n",
    "\n",
    "Determine baseline performance. Classify a test set of stand-alone question utterances, with a training set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dca39-2b52-4588-bdc1-effca951dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ff4ba-9330-431c-a3e1-8544bcb04074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d02a388-592e-4162-a231-2540d3612b1d",
   "metadata": {},
   "source": [
    "# Without Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb3dfb5-1371-448e-8959-9fbc6a787b4e",
   "metadata": {},
   "source": [
    "## Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b337c1-cd11-4d08-8826-b86a93a20b88",
   "metadata": {},
   "source": [
    "### User message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ac6263-1157-4194-b435-0efeeef420d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 544\n"
     ]
    }
   ],
   "source": [
    "prompt_no_train = f\"Classify each of the questions below, delimited by triple backticks, using the taxonomy proposed by Eris. \\\n",
    "Label each question with one of the three categories: Low-level questions, Deep Reasoning Questions, or Generative Design Questions. \\\n",
    "State your reasoning for the assigned label. Format the result as a markdown table.\\n\\n \\\n",
    "```\\n{num_ques_list}\\n```\"\n",
    "\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_no_train))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb99e6d-d721-4e0e-9623-1b1bc0aacc6b",
   "metadata": {},
   "source": [
    "## Experiment 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d4eb83-067c-4f6e-abd1-f02dc9830adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_no_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b2fc73-6b0f-4c7c-b3a5-bdfc288b60a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0209b0-d41c-42cb-a730-d447b85385c7",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84290713-674e-4abb-8be6-90faefef9c55",
   "metadata": {},
   "source": [
    "| Testing set: question number and utterance                                                                                                                          | Human   | AI /woT | AI /wT |\n",
    "| ------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --- | ------- | ------ |\n",
    "| 1\\. So how do you know how much do?                                                                                                                                 | DRQ | LLQ     | DRQ    |\n",
    "| 2\\. …what are the possible contributors?                                                                                                                            | GDQ | GDQ     | GDQ    |\n",
    "| 3\\. Why would you put that on there if you could swap in the new roll in a minute?                                                                                  | DRQ | DRQ     | DRQ    |\n",
    "| 4\\. What about cleats, right cleated conveyor cleats?                                                                                                               | GDQ | GDQ     | GDQ    |\n",
    "| 5\\. But so it's full 15 or whatever, probably right?                                                                                                                | LLQ | LLQ     | LLQ    |\n",
    "| 6\\. Could you take the candy and slide it on the wrapper?                                                                                                           | LLQ | GDQ     | GDQ    |\n",
    "| 7\\. Why would I do that?                                                                                                                                            | DRQ | DRQ     | DRQ    |\n",
    "| 8\\. How they wrap candy currently?                                                                                                                                  | DRQ | LLQ     | DRQ    |\n",
    "| 9\\. How does the very last one behave?                                                                                                                              | GDQ | LLQ     | DRQ    |\n",
    "| 10\\. But this whole thing has to rotate, right?                                                                                                                     | LLQ | LLQ     | LLQ    |\n",
    "| 11\\. So when they're wrapping in one of these candies, have you seen the whole process?                                                                             | LLQ | LLQ     | LLQ    |\n",
    "| 12\\. Yeah, but I think, isn't it the same?                                                                                                                          | GDQ | LLQ     | LLQ    |\n",
    "| 13\\. So is the main assumption right now that the wrapper will stick?                                                                                               | LLQ | LLQ     | LLQ    |\n",
    "| 14\\. How are you going to intersection things that are exactly the same…?                                                                                           | GDQ | GDQ     | GDQ    |\n",
    "| 15\\. How are you going to keep rolling and pulling it?                                                                                                              | GDQ | GDQ     | GDQ    |\n",
    "| 16\\. How was the candy being ejected from your machine.                                                                                                             | DRQ | LLQ     | DRQ    |\n",
    "| 17\\. Can I do this stuff?                                                                                                                                           | LLQ | LLQ     | LLQ    |\n",
    "| 18\\. Makes sense? Right?                                                                                                                                            | LLQ | LLQ     | LLQ    |\n",
    "| 19\\. What kind of processes out there now?                                                                                                                          | DRQ | LLQ     | DRQ    |\n",
    "| 20\\. Because these paper will come on to the stage right?                                                                                                           | LLQ | LLQ     | LLQ    |\n",
    "| 21\\. …Why did you put grid?                                                                                                                                         | DRQ | DRQ     | DRQ    |\n",
    "| 22\\. What function is providing?                                                                                                                                    | DRQ | LLQ     | LLQ    |\n",
    "| 23\\. What's gonna happen?                                                                                                                                           | GDQ | DRQ     | GDQ    |\n",
    "| 24\\. So what's happening here at the end of this conveyor belt, tell me a little bit more …What would happen in terms of how this thing gets pushed in the plastic? | DRQ | LLQ     | DRQ    |\n",
    "| 25\\. …What is the purpose of this box?                                                                                                                              | DRQ | LLQ     | LLQ    |\n",
    "| 26\\. And is this manual pushing? Or .. a motor pushing?                                                                                                             | LLQ | LLQ     | LLQ    |\n",
    "| 27…How are you going to then finalize the closure?                                                                                                                  | GDQ | GDQ     | GDQ    |\n",
    "| 28. So how do we animate this?                                                                                                                                      | GDQ | GDQ     | GDQ    |\n",
    "| 29\\. Or is it something that we can outsource to a company?                                                                                                         | GDQ | LLQ     | GDQ    |\n",
    "| 30\\. Do you want to a machine to produce 15 to 20 pieces of candy per min and currently they're doing what is it?                                                   | LLQ | LLQ     | LLQ    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b07eb",
   "metadata": {},
   "source": [
    "\n",
    "- GPT-4 generated labels align more closely with the human labels when a training set is provided - 83% when training set size is 90; compared to 60% alignment where no training set is provided. Therefore, training set could be useful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
