{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aaa0703-d8aa-4f9a-b52b-6ce66a30bb77",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac4177-5a19-4ae7-8a70-4c43734d3693",
   "metadata": {},
   "source": [
    "The goal of this experiment is to determine the effect of the size of the training set on the accuracy of labelling by the GPT-4. \n",
    "\n",
    "- The labelling task described in Experiment 1 was repeated under different conditions of the size of training set, which was varied from 0 (i.e., no training) to 300 pre-labelled questions, in increments of 30.\n",
    "- The testing set was kept constant and identical to the one in Experiment 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc87efc1-4e2c-42fd-a346-b098772e9941",
   "metadata": {},
   "source": [
    "# Libraries and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "191d2949-c2b1-4401-af36-5c47a3a986aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import openai\n",
    "import tiktoken\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import calculate_openai_cost\n",
    "\n",
    "# load environment variables from .env\n",
    "load_dotenv()  \n",
    "\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Initialize the OpenAI client w/API key\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# https://platform.openai.com/docs/models\n",
    "GPT_MODEL = os.getenv(\"OPENAI_MODEL\")\n",
    "\n",
    "# https://openai.com/api/pricing/\n",
    "PROMPT_COST_PER_1000 = os.getenv(\"PROMPT_COST_PER_1000\")\n",
    "COMPLETION_COST_PER_1000 = os.getenv(\"COMPLETION_COST_PER_1000\")\n",
    "\n",
    "DATA_DIR = os.getenv(\"DATA_DIR\")\n",
    "DATA_FILE = os.getenv(\"DATA_FILE\")\n",
    "\n",
    "# dataset ref: https://doi.org/10.1017/pds.2023.100\n",
    "data_path_qa = Path(\"../\", DATA_DIR, DATA_FILE)\n",
    "\n",
    "df = pd.read_excel(data_path_qa, usecols=\"F,G,H,O\")\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "SEED = 42\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32fda76-3048-4960-818a-43d45c268547",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db66baa0-df2d-40f1-b998-f212f35ca5a7",
   "metadata": {},
   "source": [
    "[Relevant Paper](https://doi.org/10.1017/pds.2023.100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78a76734-aa99-4ad4-9240-989157ba3d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create separate DataFrames for each class\n",
    "llq_data = df[df['A-General Type of Questions'] == 'LLQ']\n",
    "gdq_data = df[df['A-General Type of Questions'] == 'GDQ']\n",
    "drq_data = df[df['A-General Type of Questions'] == 'DRQ']\n",
    "\n",
    "# Sample 1/3 of LLQ, 1/3 of GDQ, and 1/3 of DRQ for training\n",
    "llq_train, llq_test = train_test_split(llq_data, test_size=0.3, random_state=SEED)\n",
    "gdq_train, gdq_test = train_test_split(gdq_data, test_size=0.3, random_state=SEED)\n",
    "drq_train, drq_test = train_test_split(drq_data, test_size=0.3, random_state=SEED)\n",
    "\n",
    "# Concatenate the sampled data for testing\n",
    "test_sample = pd.concat([llq_test.head(10), gdq_test.head(10), drq_test.head(10)])\n",
    "test_sample = test_sample.sample(frac=1, random_state=SEED).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72aee26-0e6e-42ba-8ce6-98b8d0b2d88a",
   "metadata": {},
   "source": [
    "# Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c5d76-962d-49c2-abee-3b52496b2bf4",
   "metadata": {},
   "source": [
    "## System message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffd373a6-c773-4121-bf45-065d3ee88db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Prompt Message Input token size: 1006\n"
     ]
    }
   ],
   "source": [
    "encoding = tiktoken.encoding_for_model(GPT_MODEL)\n",
    "\n",
    "with open(\"../system-message.txt\") as f:\n",
    "    persona_text = f.read()\n",
    "    print(f\"System Prompt Message Input token size: {len(encoding.encode(persona_text))}\")\n",
    "\n",
    "\n",
    "# NB \n",
    "# 1. The content is collected from [https://doi.org/10.1016/j.destud.2016.07.002] Appendix 1\n",
    "# 2. In the prompt, Right/Left Double Quotation Mark (“ ”) to quote Eris instead of Straight Double Quotation Mark (\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd49b3b5-4b8c-4a30-86c8-cae0d969a5a3",
   "metadata": {},
   "source": [
    "## User Message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22e865b7-4f38-4bbf-b71b-da78eaa9df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_sample(num_sample):\n",
    "    num_unit = int(num_sample / 3)\n",
    "    \n",
    "    # Concatenate the sampled data for training\n",
    "    train_sample = pd.concat([llq_train.head(num_unit), gdq_train.head(num_unit), drq_train.head(num_unit)])\n",
    "    \n",
    "    # Shuffle the training and test samples\n",
    "    train_sample = train_sample.sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "    return train_sample\n",
    "\n",
    "\n",
    "def get_user_prompt(sample_size):\n",
    "    # create train example list\n",
    "    train_sample = get_train_sample(sample_size)\n",
    "    train_sample['example'] = train_sample['Questions'] \\\n",
    "                        + ' : ' + train_sample[\"A-General Type of Questions\"] +'\\n' \n",
    "    example_txt = ''.join(train_sample['example'])\n",
    "    \n",
    "    # numbered test qs list\n",
    "    ques_list = '\\n'.join(test_sample['Questions'])\n",
    "    num_ques_list = '\\n'.join(f\"{i+1}. {question}\" for i, question in enumerate(ques_list.split('\\n')))\n",
    "\n",
    "\n",
    "    return (\n",
    "        f\"Classify each of the questions below, delimited by triple backticks, using the taxonomy proposed by Eris.\"\n",
    "        f\" Label each question with one of the three categories: Low-level questions, Deep Reasoning Questions, or Generative Design Questions.\"\n",
    "        f\" State your reasoning for the assigned label. Format the result as a markdown table.\\n\"\n",
    "        f\"```\\n{num_ques_list}\\n```\\n\"\n",
    "        f\"To help you categorize the questions above, here are some examples delimited by triple backticks, each line contains an example that has two segments - question and category separated by colon (:)\\n\\n\"\n",
    "        f\"```\\n{example_txt}```\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb38342-59a5-47a8-b630-fa2b15e1cb1d",
   "metadata": {},
   "source": [
    "# With Training Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad01ab8-5dbd-4d8e-9859-f086d3d3c6d1",
   "metadata": {},
   "source": [
    "## Sample size 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf3ad2c4-fcc3-4902-a62d-bd90da09fe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 1105\n"
     ]
    }
   ],
   "source": [
    "prompt_train = get_user_prompt(30)\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dca39-2b52-4588-bdc1-effca951dab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8ff4ba-9330-431c-a3e1-8544bcb04074",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c65116-50a7-45c6-84ac-4e3db2ddb1f7",
   "metadata": {},
   "source": [
    "## Sample size 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f2a12d00-6732-4f86-858d-04ea28e40423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 1596\n"
     ]
    }
   ],
   "source": [
    "prompt_train = get_user_prompt(60)\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c24866-57dd-474b-9e5f-dbb35a00c24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f2943-bbb5-4299-a8b8-4731aac900f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f249c419-2cb2-4600-8578-0797f2248e27",
   "metadata": {},
   "source": [
    "## Sample size 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "cc86069c-19a9-40d6-88f1-420795bb2093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 2118\n"
     ]
    }
   ],
   "source": [
    "prompt_train = get_user_prompt(90)\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b4d49-5608-4626-98ad-d711cf04369d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29f3890-6a02-43c0-8cf2-39a107b39222",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5beadb2a-9a57-499d-a3ee-c9b644bc7c3e",
   "metadata": {},
   "source": [
    "## Sample size 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "27554a54-5c93-4c66-b57f-1f7462c543a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 2556\n"
     ]
    }
   ],
   "source": [
    "prompt_train = get_user_prompt(120)\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb778279-ab5c-45e6-9642-1eb7d40f94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de36fd3-1afb-4f2f-99dd-96a74a25a332",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336cf47d-190e-4408-908d-6f26bd996e2e",
   "metadata": {},
   "source": [
    "## Sample size 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "23f5749d-b14b-436d-970b-85075a2fa403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 3134\n"
     ]
    }
   ],
   "source": [
    "prompt_train = get_user_prompt(150)\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524d3b38-857a-4bce-b7c7-af46897a90d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065ff33-ff69-4753-89bc-b46c606008a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cea891d-4717-4d11-a586-b0b9ad2aed58",
   "metadata": {},
   "source": [
    "## Sample size 180"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6f342fd8-a716-4187-891e-afe1497733be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 3605\n"
     ]
    }
   ],
   "source": [
    "prompt_train = get_user_prompt(180)\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de71b0b6-aa90-40d8-a587-e4e25d829101",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f48a4bc-1003-414e-a7cb-47d52e658d92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c9ccef-30cd-426a-b9a3-d3c00719edc3",
   "metadata": {},
   "source": [
    "## Sample size 210"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "80a1c604-6e8f-469d-a64f-c140285cdc59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 4093\n"
     ]
    }
   ],
   "source": [
    "prompt_train = get_user_prompt(210)\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3685e48-3908-4109-b7cb-909e779fc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ff6a93-4f10-4e83-b5a3-813136ec1ad6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964924e-15fb-4b16-85f3-36593406731c",
   "metadata": {},
   "source": [
    "## Sample size 240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "6b3ea435-fb1c-41b9-91a6-d5ff28cfd541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 4644\n"
     ]
    }
   ],
   "source": [
    "prompt_train = get_user_prompt(240)\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6025ff-604c-45a6-ab91-f82b5b5df800",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817ce143-a7dd-41d3-983e-e613625716a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115654cb-4a11-4d93-8a41-33f9ea2301ac",
   "metadata": {},
   "source": [
    "##  Sample size 270"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "01b27543-0cc0-4bb9-8ec8-4a9a57e857ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 5258\n"
     ]
    }
   ],
   "source": [
    "prompt_train = get_user_prompt(270)\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca65cc2-0b8a-4151-9d5b-3cea28fe57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81cc05c-9b70-4dc0-a65e-4e3a42a9303e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57107e92-a57a-4bf6-bf43-e5631bc978b8",
   "metadata": {},
   "source": [
    "## Sample size 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "36b5793d-7cbf-45a4-baf6-f6336cc4133f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Prompt Message Input token size: 5867\n"
     ]
    }
   ],
   "source": [
    "prompt_train = get_user_prompt(300)\n",
    "print(f\"User Prompt Message Input token size: {len(encoding.encode(prompt_train))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ef73c5-ca2e-43d2-9f2f-49d14d5bcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "c = openai.chat.completions.create(\n",
    "    model=GPT_MODEL,\n",
    "    n=1,\n",
    "    seed=SEED,\n",
    "    temperature=0,\n",
    "    messages=[{\"role\": \"system\", \"content\": persona_text},\n",
    "              {\"role\": \"user\", \"content\": prompt_train}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0296b-a14c-471f-86ec-162fdb764f56",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f\"System Fingerprint: {c.system_fingerprint}\\n\")\n",
    "print(c.choices[0].message.content)\n",
    "\n",
    "cost, prompt_tokens, completion_tokens, total_tokens = calculate_openai_cost(c.usage, PROMPT_COST_PER_1000, COMPLETION_COST_PER_1000)\n",
    "\n",
    "print(f\"Total cost: ${cost:.5f}\")\n",
    "print(f\"Prompt tokens: {prompt_tokens}\")\n",
    "print(f\"Completion tokens: {completion_tokens}\")\n",
    "print(f\"Total tokens: {total_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031d4120",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd454b43",
   "metadata": {},
   "source": [
    "<table border=\"1\" cellpadding=\"10\" cellspacing=\"0\">\n",
    "  <tr>\n",
    "    <th rowspan=\"2\"></th>\n",
    "    <th colspan=\"11\">Size of training set (# of human-labelled questions)</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>0 (No training)</th>\n",
    "    <th>30</th>\n",
    "    <th>60</th>\n",
    "    <th>90</th>\n",
    "    <th>120</th>\n",
    "    <th>150</th>\n",
    "    <th>180</th>\n",
    "    <th>210</th>\n",
    "    <th>240</th>\n",
    "    <th>270</th>\n",
    "    <th>300</th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <th>Alignment with human-assigned labels (%)</th>\n",
    "    <td align=\"center\"><span style=\"border: 1px solid purple; border-radius: 50%; padding: 4px;\">60</span></td>\n",
    "    <td align=\"center\">67</td>\n",
    "    <td align=\"center\" style=\"background-color: gray;\"><span style=\"border: 1px solid purple; border-radius: 50%; padding: 4px;\">83</span></td>\n",
    "    <td align=\"center\" style=\"background-color: gray;\"><span style=\"border: 1px solid purple; border-radius: 50%; padding: 4px;\">83</span></td>\n",
    "    <td align=\"center\">80</td>\n",
    "    <td align=\"center\">83</td>\n",
    "    <td align=\"center\">83</td>\n",
    "    <td align=\"center\">73</td>\n",
    "    <td align=\"center\">70</td>\n",
    "    <td align=\"center\">73</td>\n",
    "    <td align=\"center\">83</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f667d1c",
   "metadata": {},
   "source": [
    "- GPT-4 generated labels align more closely with the human labels when a training set is provided - 83% when training set size is 60 or 90; compared to 60% alignment where no training set is provided.\n",
    "- No accuracy improvements are achieved when the size is increased past 90 questions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
